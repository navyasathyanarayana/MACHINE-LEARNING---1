{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are the three stages to build the hypotheses or model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soln:\n",
    "The process of getting data ready for Machine Learning algorithm can be summarized in three steps:\n",
    "\n",
    "a)Model building\n",
    "\n",
    "b)Model testing\n",
    "\n",
    "c)Applying the model\n",
    "\n",
    "Note:\n",
    "Select Data:\n",
    "This step is concerned with selecting the subset of all available data that you will be working with. There is always a tendency for including all data that is available, that the maxim “more is better” will hold. This may or may not be true.\n",
    "\n",
    "Preprocess Data:\n",
    "After you have selected the data, you need to consider how you are going to use the data. This preprocessing step is about getting the selected data into a form that you can work.Three common data preprocessing steps are formatting, cleaning and sampling\n",
    "\n",
    "Transform Data:\n",
    "The final step is to transform the processed data. The specific algorithm you are working with and the knowledge of the problem domain will influence this step and you might have to revisit different transformations of your preprocessed data as you work on your problem. Three common data transformations are scaling, attribute decompositions and attribute aggregations. This step is also referred to as feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.What is the standard approach to supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard approach to supervised learning is to split the set of example into the training set and the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Supervised learning algorithms are trained using labeled examples, such as an input where the desired output is known. For example, a piece of equipment could have data points labeled either “F” (failed) or “R”(runs). The learning algorithm receives a set of inputs along with the corresponding correct outputs, and the algorithm learns by comparing its actual output with correct outputs to find errors. It then modifies the model accordingly. Through methods like classification, regression, prediction and gradient boosting. Supervised learning uses patterns to predict the values of the label on additional unlabeled data. Supervised learning is commonly used in applications where historical data predicts likely future events. For example, it can anticipate when credit card transactions are likely to be fraudulent or which insurance customer is likely to file a claim. Supervised algorithms can further be divided into following:\n",
    "\n",
    "1.Classification: When the data is being used to predict a category, supervised learning is also called classification. This is the case when assigning an image as a picture of either a 'cat' or a 'dog'. When there are only two choices, it's called two-class or binomial classification. When there are more categories, like predicting the winner of the NCAA March Madness tournament, this problem is known as multi-class classification.\n",
    "\n",
    "2.Regression: When a value is being predicted, as with stock prices, supervised learning is called regression.\n",
    "\n",
    "3.Anomaly detection: Sometimes the goal is to identify data points that are simply unusual. In fraud detection, for example, any highly unusual credit card spending patterns is considered to be a suspect. The possible variations are so numerous and the training examples so few, that it's not feasible to learn what fraudulent activity looks like. The approach that anomaly detection takes is to simply learn what normal activity looks like (using a history of non-fraudulent transactions) and identify anything that is significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.What is Training set and Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In various areas of information science like machine learning, a set of data is used to discover the potentially predictive relationship known as ‘Training Set’. Training set is an examples given to the learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of example held back from the learner. Training set are distinct from Test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Training Set:In Machine Learning, a training set is a dataset used to train a model. In training the model, specific features are picked out from the training set. These features are then incorporated into the model. Thereby, if the\n",
    "training set is labeled correctly, the model should be able to learn something from these features.\n",
    "\n",
    "Test Set:The test set is a dataset used to measure how well the model performs at making predictions on that test set.\n",
    "If the prediction scores for the test set are unreasonable, we’ll have to make some adjustments to our model\n",
    "and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model.  Bagging is a method in ensemble for improving unstable estimation or classification schemes.  While boosting method are used sequentially to reduce the bias of the combined model.  Boosting and Bagging both can reduce errors by reducing the variance term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "General principle of an ensemble method - \n",
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model.\n",
    "\n",
    "Bagging:\n",
    "Bagging is a method in ensemble for improving unstable estimation or classification schemes. Bagging both can reduce errors by reducing the variance term.\n",
    "\n",
    "Boosting:\n",
    "boosting method are used sequentially to reduce the bias of the combined model. Boosting can reduce errors by reducing the variance term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a lot of data overfitting can be avoided, overfitting happens relatively as you have a small dataset, and you try to learn from it. But if you have a small database and you are forced to come with a model based on that. In such situation, you can use a technique known as cross validation. In this method the dataset splits into two section, testing and training datasets, the testing dataset will only test the model while, in training dataset, the datapoints will come up with the model.\n",
    "\n",
    "In this technique,  a model is usually given a dataset of a known data on which training (training data set) is run and a dataset of unknown data against which the model is tested. The idea of cross validation is to define a dataset to “test” the model in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function. As such, many nonparametric Machine Learning algorithms also include parameters or techniques to limit and constrain how much detail the model learns. For example, decision trees are a nonparametric Machine Learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.\n",
    "\n",
    "Over time, as the algorithm learns, the error for the model on training data goes down and so does the error\n",
    "on the test dataset. If we train for too long, the performance on the training dataset may continue to decrease\n",
    "because the model is overfitting and learning irrelevant details and noises in the training dataset. At the same\n",
    "time the error for the test set starts to rise again as the model’s ability to generalize decreases.\n",
    "The sweet spot is the point just before the error on the test dataset starts to increase where the model has\n",
    "good skill on both the training dataset and the unseen test dataset.\n",
    "You can perform this experiment with your favorite Machine Learning algorithms. This is often not useful\n",
    "technique in practice, because by choosing the stopping point for training using the skill on the test dataset\n",
    "means that the testset is no longer “unseen” or a standalone objective measure. Some knowledge (a lot of\n",
    "useful knowledge) about that data has leaked into the training procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
